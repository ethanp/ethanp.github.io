<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Distributed Systems | With Pith]]></title>
  <link href="http://ethanp.github.io/blog/categories/distributed-systems/atom.xml" rel="self"/>
  <link href="http://ethanp.github.io/"/>
  <updated>2016-05-12T15:16:35-07:00</updated>
  <id>http://ethanp.github.io/</id>
  <author>
    <name><![CDATA[Ethan Petuchowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bigtable Paper Summary]]></title>
    <link href="http://ethanp.github.io/blog/2016/04/10/bigtable-paper-summary/"/>
    <updated>2016-04-10T22:02:12-07:00</updated>
    <id>http://ethanp.github.io/blog/2016/04/10/bigtable-paper-summary</id>
    <content type="html"><![CDATA[<p>When looking into what Cassandra and HBase are, and their relative strengths
and weaknesses, people often seem to think they can get away with the
following very succinct characterizations: &ldquo;Cassandra is like is Dynamo plus
Bigtable, and HBase is just Bigtable&rdquo;. I don&rsquo;t know much about Dynamo <em>or</em>
BigTable because we skipped those papers in my systems courses. So to get
started understanding what&rsquo;s going on with all this mess, I decided to read
the Bigtable paper. What follows is a brief summarization/retelling of the
<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable paper</a>. It follows roughly the form of the paper, especially in
that it starts high level, and then digs slightly lower-down into the
implementation. It contains basically only and all of the parts of the paper
that I found illuminating, but broken down into sentences that are hopefully
easier to understand.</p>

<h3>What problem are we solving?</h3>

<p>Bigtable provides an API for storing and retrieving data.</p>

<p>It is most useful if</p>

<ul>
<li>there is a <em>lot</em> of data coming in at a high rate over time</li>
<li>there is no need to join each data table with another</li>
<li>data might need to be updated</li>
<li>range queries are common</li>
</ul>


<p>Bigtable is a distributed database. It is a database management system which
allows you to define tables, write and update data, and run queries against
the stored data. It is similar to a relational database, except that it is not
&ldquo;relational&rdquo; in the sense denoted by the term &ldquo;relational database&rdquo;. Instead,
it brings its own type of data model, where instead of storing data in normal
two-dimensional table cells, you store it according to a new set of rules that
allows for flexibility in the shape of each record, while still enabling
overall efficiency.</p>

<!-- more -->


<h3>Go on&hellip;</h3>

<p>In the Bigtable data model, data is grouped in tables. Each record in one
table must have a string identifier unique in that table. Each table is
defined to have a set of &ldquo;column families&rdquo;. This is where the data goes. A
record has values, that is the point of having a record. Each of those values
is associated with a timestamp when it was created. This can either be defined
by Bigtable or by the user inserting the data. Having two versions of a record
means it was updated. Each value has its own key (a string) pointing to it.
That key has two components, a &ldquo;family&rdquo; and a &ldquo;qualifier&rdquo;. Each table has a
finite set of families with known names. However, the qualifier segment of a
value&rsquo;s key may be an arbitrary string. There may be any number of anchors for
each family in each record. You can hint whether you want Bigtable to prefer
disk locality when writing many values in the same column family.</p>

<p>Rows are stored on disk in alphabetically by row key, and for each value, in
<em>descending</em> order of timestamps (i.e. most recent first). All reads and
writes over a single key are atomic with respect to each other, but not with
respect to changes in other rows. (This is something the client must be
careful about.) All the rows for a given table are chopped up (dynamically)
into &ldquo;tablets&rdquo; which each are stored in by exactly one &ldquo;tablet server&rdquo; node in
the cluster. One can specify garbage collection to either only keep the last
<em>n</em> versions, or to only keep everything for a specified duration.</p>

<p>When reading the data, one might e.g. just fetch the data for one column
family within a particular range of row names, but get all currently known
versions of that data. This type of query is what was optimized by the
creators of Bigtable. One might for example want to execute this type of query
to feed as input to a MapReduce job. One could do that, and then also be able
to enjoy the intentionally efficient writing of MapReduce results into a
different Bigtable table.</p>

<p>Bigtable data is stored in the SSTable file format. In this format, each file
represents what in Scala would be called a <code>immutable.Map[String, String]</code>. A
file is made of blocks, each containing a continuous interval of the keys that
is listed in the file&rsquo;s &ldquo;block index&rdquo;. Each SSTable file is stored in Google
File System (GFS), which I will not describe in detail here, and is similar to
the Hadoop File System (HDFS).</p>

<p>Bigtable relies on Chubby (similar to Apache Zookeeper) to ensure there is &ldquo;at
most one active aster at any time&rdquo; and for bootstrapping.</p>

<p>A Bigtable instance has a single master node, and a potentially dynamic number
of tablet servers. A tablet is a complete alphabetical interval of all the
data in a table. The master manages tablet assignment, server cluster
membership changes (with help from Chubby), load-balancing of tablets among servers, garbage
collection, and schema changes. The tablet server handles read and write
requests to its tablets, and splits tablets when they get too big.</p>

<p>Clients only need to communicate with the master to find the relevant tablet
servers. The relative amount of this sort of communication compared to the
actual data transfers is such that this practice is not a system-wide
bottleneck. Clients also cache this data until a server says the information
is stale.</p>

<p>Tablet location information (across all tables being managed by a single
Bigtable instance) is stored in one big 3-level hierarchical B+-tree type
thing (i.e. it&rsquo;s a sorted <em>n</em>-way tree, where you go down the appropriate
pointer from an internal node based on what it says about the interval of row
IDs contained in its leaves. The root is stored in Chubby. At the third level
I think you have the locations of all the tablets and what keys are owned by
each.</p>

<p>As previously mentioned, persistent tablet state is stored in GFS, but what is
that &ldquo;tablet state&rdquo;. Each write or update is appended a commit log (with a
unique mutation identifier to resolve duplication caused by tricky crash
scenarios). It also modifies the (copy-on-write) in-memory representation of
the current state of this chunk of the table. One can always reconstruct the
in-memory representation by playing back the commit log, but ideally most
reads are serviced from memory. Every once in a while the in-memory state gets
too big, and <em>it</em> is written out as an SSTable into GFS. Every once in a
while, multiple SSTables are read back into, and since some might be updates
to others, they are merged together, and the sources deleted.</p>

<p>In practice, the authors found that applying a very computationally efficient
compression algorithm (rather than one with a top-notch compression ratio)
increased overall system efficiency in many cases. They also added bloom
filters to the SSTables to reduce the number of unecessary disk reads of non-
existent data.</p>

<p>The paper says Google has used Bigtable as a backend for its Google Analytics
product, Google Earth, Personalized Search, and storing websites for
retrieving results for its Search Engine.</p>

<p>As future work they want to be able to provide better (but not full) support
for transactions (I think this is addressed in the Spanner paper released a
few years later).</p>

<p>Their lessons learned include</p>

<ul>
<li>&ldquo;delay adding new features until it is clear how the new features will be
used&rdquo;</li>
<li>&ldquo;the importance of proper system-level monitoring&rdquo;</li>
<li>&ldquo;the value of simple designs&hellip;code and design clarity are of immennse
help in code maintenance and debugging&rdquo;</li>
</ul>


<p>They conclude by noting that this system bears some resemblance to the C-Store
column-oriented database system, for which there is another good paper that I
ought to finish reading at some point. They also note that people who are used
to &ldquo;traditional&rdquo; relational databases find the Bigtable interface awkward, but
that it works so much better than the alternatives that people end up using it
anyway.</p>

<h3>References</h3>

<ul>
<li>Chang, Fay, et al. &ldquo;Bigtable: A distributed storage system for structured
data.&rdquo; ACM Transactions on Computer Systems (TOCS) 26.2 (2008): 4.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Summary of 'the Log']]></title>
    <link href="http://ethanp.github.io/blog/2015/05/29/summary-of-the-log/"/>
    <updated>2015-05-29T20:57:16-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/05/29/summary-of-the-log</id>
    <content type="html"><![CDATA[<p>I read a great article by Jay Kreps, one of the dudes who brought you Apache
Kafka. The article is called <a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"><strong>The Log</strong>: What every software engineer should know about real-time data&rsquo;s unifying abstraction</a>. The
article took me a few hours of mildly challenging reading, so I figure you can
spend a few minutes reading my summary and decide more clearly whether you want
to put in the investment of his explanation.</p>

<p>First, he describes the following concept: Let&rsquo;s define a <strong>log</strong> as a <em>total
order</em> of functions called and the parameters passed to each of those function-
calls. Let&rsquo;s define a <em>&ldquo;commit log&rdquo;</em> as a <em>log</em> of edits of the contents of a
database. Kreps notes that we can use a commmit log to build the state of a
database at any point in history. This is like <code>git</code>; as we patch in each
commit, we obtain the state of the repo at each time. If we feed the same log
to the same program on multiple machines in a cluster, (assuming none of the
functions called are <em>non- deterministic</em> and machines behave as we expect), we
will certainly have the same state on each machine after they have each
executed the log. This would be desireable perhaps to provide a &ldquo;reliable&rdquo;
<em>service</em> for which there are more reads than writes, and more reads than can
be handled by one node; then we can ensure any node is OK to read from by
having all nodes play functions as prescribed by the log.</p>

<p>Then, he describes the following situation: getting every part of a tech-
company&rsquo;s data to every service that needs it is very complicated. In the worst
and most naive case it would be <em>N<sup>2</sup></em> because each of <em>N</em> places would be sending
data to each of <em>N</em> places. That&rsquo;s a lot of network bandwidth, and complexity,
and data formats to understand, and places for things to screw up. So Kreps
suggests</p>

<!-- more -->


<p>just having all data producers standardize a framework for formatting
the data they produce, then just have all producers append to a single shared
log. Now, everyone who wants to read data from someone else can be sure to get
that data in the order it was produced by reading from that one log. Plus there
are N writers and N readers so getting the data to every where is <em>2N</em>. Now we
note that 2N &lt; N<sup>2</sup> if N > 2, so most of the time this is advantageous.</p>

<p>Now, the problem is that this single log is not going to be able to handle that
throughput, and it is going to get way too big way too fast. To build Kafka as
an implementation of this &ldquo;unified log&rdquo; concept, the key optimization is to
&ldquo;partition&rdquo; this log, meaning different pieces of it are written to different
places (machines), and each piece is written in duplicate to multiple machines.
In general we lose the total ordering across all processes, but in general,
this total ordering was literally more strict than could possibly be useful.</p>

<blockquote><p>This is the point where I stopped paying as much attention.</p></blockquote>

<p>Then Kreps goes on to talk about how important stream-processing is, because so
many of the services modern tech companies provide operate on a real-time feed
of events. Then he notes that log-table duality noted in the second paragraph
allows us to provide a reliable enriched event stream, ie. one that takes raw
events, joins each one with data from another table, and inserts some
maintained state like a counter.</p>

<p>Then Kreps notes that Kafka&rsquo;s cleverest provided algorithm for freeing up log
space, is to remove &ldquo;records whose primary key has a more recent update.&rdquo; The
naive provided algorithm is to discard elements that are more than <em>x</em> days
old.</p>

<p>Then he notes how most companies just exist to manipulate data in a distributed
system, and how building distributed systems in Java has to a large extent
become a problem of putting open source lego blocks like Zookeeper, Kafka,
Netty, etc. together. After that he summarizes and concludes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paxos as a Classroom]]></title>
    <link href="http://ethanp.github.io/blog/2015/05/18/paxos-as-a-classroom/"/>
    <updated>2015-05-18T17:28:49-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/05/18/paxos-as-a-classroom</id>
    <content type="html"><![CDATA[<pre><code>multi-paxos as a classroom:
teacher := proposer
students := acceptors
all := learners

teacher to students:
    may I have your attention for the next 1:15 hrs?
    I'd like to teach youz guys all about "the paxos protocol"
    here is what I'm assuming you already know

each student to teacher:
    if not already busy for that time:
        yes, here's how much I actually already know about paxos
    else:
        my apologies, but I will be busy skimming facebook for the next 1:15
        so if there's something you want to tell me, post it on there

teacher to each student:
    given how far behind you are, these are the things you need to catch up on
    but also this is what I'm teaching now

each (alive &amp; correct) student to teacher:
    oh very cool!
    this is how much I now understand of all the stuff you've said

teacher:
    if the majority understands more than before:
        set new start point for next time's lesson
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simulating a Distributed System]]></title>
    <link href="http://ethanp.github.io/blog/2015/05/18/simulating-a-distributed-system/"/>
    <updated>2015-05-18T17:24:37-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/05/18/simulating-a-distributed-system</id>
    <content type="html"><![CDATA[<p>For my Distributed Computing class at school, there were 3 projects that each
involved implementing a distributed protocol over a simulated distributed
cluster of computers that are only &lsquo;connected&rsquo; to one another in that they know
each other&rsquo;s IP address and must communicate over TCP sockets. For each of the
3 projects, I had a <em>completely</em> different method of implementing the system.</p>

<h3>Implementation One: A True Distributed System in Vanilla Java</h3>

<p>The first project (Three Phase Commit) was the only one for which I worked with
a partner. We were unclear how to attack the problem of building a distributed
system in a simple way, so we went ahead and implemented a truly distributable
Java application, in which you could spin up processes on any computer and it
would connect in to the master server, who would tell the new node the
listening ports of other servers in the cluster manually, etc. This turned out
to be perhaps overly ambitious. When a node was expecting a message from
someone else, it would fire a timer thread, which on completion would report to
the master to restart the failed process. This system did work properly
(somehow) but it seemed to be living-on-the-edge. It also required two weeks of
a lot of working on it and learning aspects of Java, unit testing, mocks, which
<code>Exception</code> would get thrown by which network event &amp; when, etc.</p>

<h3>Implementation Two: Lightweight System Simulation</h3>

<p>For the second project (Paxos), I said &ldquo;Hooey&rdquo; to all the Java hubbub, which
while I enjoyed its low-level-ness, forcing me to peak further under the hood,
it required a larger time commitment than I was prepared to make. I realized
that my class&rsquo;s requirements only specified that system &ldquo;nodes&rdquo; run
concurrently, share no memory, and talk over TCP. So in Scala, I made a <code>Node</code>
class state machine, and ran different instances of it on separate threads.
This was too easy though, and I was disappointed to find my implementation
complete more than two weeks before the due date.</p>

<h3>Implementation Three: Akka Cluster</h3>

<p>Akka Cluster is a Scala-based framework for building a distributed system, in
which you define the address of a set of &ldquo;seed&rdquo; nodes, of whom at least one is
supposed be available at all times. Then when other nodes start up, they become
a &ldquo;member&rdquo; of the cluster by contacting any seed node. So for me (the &ldquo;client&rdquo;
of Akka Cluster), all there is left to do is define the protocol to run on top
of the cluster. (In this case the protocol was the simple eventually-consistent
database protocol &ldquo;Bayou&rdquo;.)</p>

<p>Akka nodes follow the &ldquo;Actor Model&rdquo;, meaning they share no state with any other
part of your program, making them &ldquo;simple&rdquo; to run on remote machines (I didn&rsquo;t
try that, the docs say it is easy though). The only way actors can communicate
is via message passing, for which there is a dedicated syntax, <code>!</code>. For
example:</p>

<p><code>scala
case class MyMessage(data: DataElem)
anotherActor ! MyMessage(theData)
</code></p>

<p>The code above would have the actor running this code, send <code>anotherActor</code> an
instance of the case class <code>MyMessage</code> with the <code>DataElem</code> that was passed in.</p>

<p>Inter-actor communcation is guaranteed <em>FIFO</em> by Akka, meaning that for any
pair of actors <code>A, B</code>, if <code>A</code> sends <code>n</code> messages to <code>B</code>, <code>B</code> will receive those
messages in the same order that <code>A</code> sent them. NB this says nothing about the
order with respect to other actors in the system.</p>

<p>Using Akka trivialized all sorts of implementation details I&rsquo;d had to worry
about in the first project, and then ignored for the second project.</p>

<p>If there had been a 4th project for this class, it&rsquo;s clear to me that I would
use Akka again. For some reason my code ran <em>very</em> slowly, and it probably had
to do with some configuration-setting that I did not set in the Akka config
file. That was really the only problem I had that I did not fix. One problem I
did fix that took a while, was figuring out a good way to shut the system down.
There were various options discussed online, and eventually I found one that
worked for me, which was to call <code>system.shutdown()</code> on every instance System I
instantiate. Another option was to send <code>self ! PoisonPill</code> but for some reason
that didn&rsquo;t work. I guess different ones work for different situations.</p>
]]></content>
  </entry>
  
</feed>
