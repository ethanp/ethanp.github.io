<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Networking | With Pith]]></title>
  <link href="http://ethanp.github.io/blog/categories/networking/atom.xml" rel="self"/>
  <link href="http://ethanp.github.io/"/>
  <updated>2016-04-10T22:05:56-07:00</updated>
  <id>http://ethanp.github.io/</id>
  <author>
    <name><![CDATA[Ethan Petuchowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[What Actually Is SSH]]></title>
    <link href="http://ethanp.github.io/blog/2016/03/06/what-actually-is-ssh/"/>
    <updated>2016-03-06T14:18:53-08:00</updated>
    <id>http://ethanp.github.io/blog/2016/03/06/what-actually-is-ssh</id>
    <content type="html"><![CDATA[<h3>SSH Tunneling</h3>

<p><em>&ldquo;Tunnelling&rdquo;, with two ells, is the British spelling.</em></p>

<p>A few months ago, I downloaded a tool (an ELK stack) that didn&rsquo;t work right
off the bat due to some sort of misconfiguration. It was running in a Vagrant-
made virtual machine (VM) on my laptop. The Vagrant setup script had forwarded
a local port on my laptop into the VM. So in order to debug it, my coworker
set up a reverse port-forward from my laptop onto an internal server. Then he
set up a forward port-forward from his laptop to that server. Now commands
issued at his laptop&rsquo;s terminal were being executed on the VM on my laptop. My
mind was so utterly blown I became dizzy.</p>

<p>It took me weeks to figure out what the SSH port-forwarding is and how its
syntax works, and another few weeks to figure out what reverse port-forwarding
is, and another few weeks to find practical use-cases for each.</p>

<p>Here is my executive summary</p>

<pre><code>ssh -L [&lt;localhost&gt;:]&lt;localport&gt;:&lt;remotehost&gt;:&lt;remoteport&gt; &lt;gateway&gt;
</code></pre>

<p>By default, <code>&lt;localhost&gt;</code> will be <code>localhost</code>.</p>

<p>What this does is, start a serversocket listening to local address
<code>localhost:localport</code> using the &ldquo;SSH client&rdquo;. When a client establishes a
connection to that address, traffic received from that client will be
encrypted, and forwarded to the <code>sshd</code>[aemon] listening on port 22 of
<code>gateway</code>. (Only) after the gateway receives this traffic, the <code>sshd</code> will
establish a (normal, unencrypted) connection to remote address
<code>remotehost:remoteport</code>, and forward the data originally received by the SSH
client there. Response traffic originating from <code>remotehost:remoteport</code> will
go back to the <code>sshd</code>, back through the encrypted tunnel to the SSH client,
and back to the originating client.</p>

<p>That&rsquo;s a mouthful, but I think it&rsquo;s a pretty clear explanation.</p>

<p><em>Reverse</em> tunneling by contrast, means that traffic <em>originating</em> on the
remote end will be forwarded to the local end.</p>

<h2>What&rsquo;s SSH</h2>

<p>One thing that confused me about SSH forwarding, is that if I don&rsquo;t use some
extra flags to disable it, when I set up port forwarding to a another machine,
I also end up with a shell ready executing commands remotely. What is going on
here? What does it mean to do remote command execution?</p>

<p>So here&rsquo;s what&rsquo;s happening: SSH is capable of multiplexing multiple
communication &ldquo;channels&rdquo; over a single encrypted TCP connection. If you just
execute the normal tunneling command given above, it will create two channels,
one for forwarding the tcp traffic, and another for executing commands
visually in your pseudo-terminal but physically in a remote shell.</p>

<p>When you <code>ssh</code> onto a remote machine, a particular sequence of things happen
that I think really clarify what the heck&rsquo;s going on.</p>

<ol>
<li>You connect via TCP to the standard SSH-daemon port (22) on the remote
machine</li>
<li>You authenticate it via its public key

<ul>
<li>That&rsquo;s why the first time you connect is says &ldquo;do you want to trust this
server&rsquo;s fingerprint?&rdquo; or something like that (I think saying &ldquo;yes I
trust it&rdquo; implies you have verified that the fingerprint does not belong
to a <em>man-in-the-middle</em> attacker)</li>
</ul>
</li>
<li>It authenticate&rsquo;s you, either by you supplying a password, or by it finding
your username in its <code>~/.ssh/authorized_keys</code> file, and having you match it
with your private key</li>
<li>Y'all negotiate an encryption for the session, over which future
communications will take place</li>
<li>Now that communications are encrypted, control and channel data are
transferred using a binary packet format that contains

<ul>
<li>Encrypted packet length</li>
<li>Various random paddings</li>
<li>The encrypted (and possibly compressed) payload</li>
<li>A message authentication code (MAC)</li>
<li>Recipient&rsquo;s integer identifier for this channel</li>
</ul>
</li>
<li>To setup a remote shell, your SSH client requests to create a &ldquo;shell&rdquo; channel

<ul>
<li>At this point you tell the server which ID you&rsquo;re allocating to this
channel</li>
<li>Since each channel has flow-controlling, you also state your window size

<ul>
<li>Both sides maintain a receive window, and once the sender has sent
enough bytes to fill it, they will not send more data until you say
that your window now has more room</li>
</ul>
</li>
</ul>
</li>
<li>If the server supports &ldquo;shell&rdquo; channels, it will tell you which ID it has
allocated to this channel, so that you can put the ID in SSH packets in
this channel</li>
<li>Then you tell the server your psuedo-terminal&rsquo;s display specifics
(characters horizontally and vertically, as well as pixels horizontally and
vertically)

<ul>
<li>For X11, you would tell the server your screen&rsquo;s specifics</li>
<li>If your dimensions change, you can send an update message to the server</li>
</ul>
</li>
<li>Then you can send the server the values of environment variables to use</li>
<li>Then you request that the server initiate a shell in which to execute the
commands you&rsquo;re going to give it. You can specify the path to the specific
shell to use.</li>
<li>Then you start sending commands to execute, which the server will pass to
the shell it created</li>
<li>The server will send you back output received from the shell, formatted
for your terminal</li>
<li>You can also send signals for the server to pass to its shell</li>
<li>When the remote shell command terminates, the server will send you its
&ldquo;exit-status&rdquo;, or if there was a violent termination, it will send an
&ldquo;exit-signal&rdquo;</li>
</ol>


<!-- more -->


<p>Note this communication only used one channel, and there can be however many
channels open over the same SSH connection because of the framing layer and
the channel identifiers. So another channel might be used for forwarding
packets received on a port to which the SSH program is listening, and that&rsquo;s
how we&rsquo;d end up with the &ldquo;-L&rdquo; feature described above.</p>

<h3>It&rsquo;s like HTTP/2</h3>

<p>I don&rsquo;t know many protocols, so it was fun to look into this one. I started on
the Wikipedia page, and then moved on to the IETF RFC cited by the Wikipedia
page. As I&rsquo;ve noticed many times recently, the RFC is <em>easier</em> to understand
than the Wikipedia page. The reason for that is probably similar to the reason
why the RFC is easier to understand than my explanation above, and is
potentially related to the fact that the RFC&rsquo;s authors are familiar with
explaning such concepts, and spent months working as a team to increase the
document&rsquo;s utter clarity so that there can be no miscommunication.</p>

<p>Another protocol that I have looked at is the new HTTP/2 IETF RFC. That
protocol contains a similar framing layer over TCP that allows it to multiplex
multiple channels concurrently over a single underlying TCP socket. The reason
for that was to ease issues with &ldquo;head-of-line blocking&rdquo; which is where one
channel&rsquo;s being slow ends up slowing down all the other channels because of
TCP unecessarily guaranteeing order over multiplexed channels that have no
globally-defined order. After having seen this framing-layer thing in two
protocols, now I understand it to be some sort of &ldquo;pattern&rdquo; for allowing
multiplexing communications over a single TCP link. An advantage to this is
decreased port usage on the server who may be handling multiple channels for
many different clients at once. Another advantage is that the TCP algorithm
has a &ldquo;ramp up&rdquo; time, i.e. the congestion control starts out with a rather
pessimistic window size. By using only one socket, that ramp up only must be
done once. I&rsquo;m sure there are many other advantages that if I had a better
grasp of TCP would be obvious.</p>

<p>I don&rsquo;t like using tools when I really have <em>no clue</em> what they&rsquo;re doing.
However doing so is inevitable because the modern stack of technologies is so
vast and the &ldquo;lyfe so shorte&rdquo;. Since my first computer science class I was
taught to press my &ldquo;I believe&rdquo; button when something seemed like magic. Still,
lifting some of the veil on one of the tools I use everyday is very
satisfying, and hopefully the time taken for others to get to my (still
minimal) level of understanding is reduced by the explanations above.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some Java Network Programming Fundamentals]]></title>
    <link href="http://ethanp.github.io/blog/2016/02/28/some-java-network-programming-fundamentals/"/>
    <updated>2016-02-28T15:15:13-08:00</updated>
    <id>http://ethanp.github.io/blog/2016/02/28/some-java-network-programming-fundamentals</id>
    <content type="html"><![CDATA[<p>Most of what I&rsquo;ve learned and discussed here comes from <em>TCP/IP Sockets in
Java</em>, a highly recommended book about this stuff by Calvert and Donahoo. Some
of it also comes from <em>Java Network Programming</em> by Elliotte Rusty Harold.</p>

<h2>Overview</h2>

<ul>
<li>The <em>only</em> transport-layer protocols Java supports are TCP &amp; UDP; for
anything else, you must link to native code via the Java Native Interface</li>
<li>TCP uses <em>stream</em> sockets, through which one generally just writes to an
<code>OutputStream</code> and reads from an <code>InputStream</code> of bytes that remain in-order
and uncorrupted and are (practically) guaranteed delivery by the
implementation of the protocol by the operating system.

<ul>
<li>Unless you&rsquo;re using NIO; see below for more on that</li>
</ul>
</li>
<li>UDP uses <em>datagram</em> sockets, through which you <code>send</code> and <code>receive</code> objects
called <code>DatagramPacket</code>s, which are just a length, a destination, and data</li>
<li>Unless you&rsquo;re using NIO, everything <em>blocks</em>: e.g. connecting to servers,
listening for clients, reads, writes, and disconnecting (for TCP)

<ul>
<li>By default most of these actions may block <em>indefinitely</em></li>
<li>For reading and connecting, you can configure a timeout, after which you
will receive an <code>InterruptedIOException</code></li>
<li>For <em>writing</em> to a TCP stream, you <em>cannot</em> configure a timeout</li>
</ul>
</li>
</ul>


<h3>Handling multiple clients</h3>

<ul>
<li>Deal with one at a time, which is simplest, especially if there&rsquo;s some state
that is shared by all potential clients. Speed may become problematic
quickly.</li>
</ul>


<p>```java
void mainLoop() {</p>

<pre><code>while (true) {
    Socket s = serverSocket.accept();
    handle(s);
}
</code></pre>

<p>}
void handle(Socket s) {</p>

<pre><code>InputStream in = s.getInputStream();
// process request, etc.
s.close();
</code></pre>

<p>}
```
* Create a new thread to handle each incoming client. This is still pretty
  simple, but will lead to massive overhead if you have many concurrent
  clients, and therefore you&rsquo;re context-switching all the time.</p>

<p>```java
void mainLoop() {</p>

<pre><code>while (true) {
    Socket s = serverSocket.accept();
    new Thread() {
        @Override public void run() {
            InputStream in = s.getInputStream();
            // process request, etc.
            s.close();
        }
    }.start()
}
</code></pre>

<p>}
```</p>

<ul>
<li>Use a thread pool to handle requests. Java has abstracted the thread pool
concept into the <code>Executors</code> factory class. There are a multitude of
executors to choose from. This <code>newCachedThreadPool()</code> one will execute each
task on an existing thread if one is idle, and will create a thread
otherwise. Threads sitting idle in the cache for over one minute are
terminated.</li>
</ul>


<p>```java
ExecutorService executor = Executors.newCachedThreadPool()</p>

<p>void mainLoop() {</p>

<pre><code>while (true) {
    Socket s = serverSocket.accept();
    executor.execute(new TheHandler(s));
}
</code></pre>

<p>}
static class TheHandler implements Runnable {</p>

<pre><code>Socket s;
public TheHandler(Socket s) { this.s = s; }
@Override public void run() {
    InputStream in = s.getInputStream();
    // process request, etc.
    s.close();
}
</code></pre>

<p>}
```</p>

<ul>
<li>Use NIO (rather complicated) to allow <code>N</code> threads to service <code>M</code>
clients, where <code>N</code> is small and <code>M</code> is huge. This uses event-based
programming. We can set all network operations to be non-blocking, and
only wait as long as we want to for them. An extensive example can be
found below.</li>
<li>Use a framework like Netty, Akka, etc. that wraps the NIO stuff up in a
ribbon and a tie</li>
</ul>


<h3>10K feet above NIO</h3>

<!-- more -->


<ul>
<li>If you&rsquo;re using NIO, you create <code>Channel</code>s of bytes into and out of sockets
(or file handles)</li>
<li>You register a <code>Selector</code> to be notified when the <code>Channel</code> is ready to be
read from or written to</li>
<li>You query the <code>Selector</code> to tell you which <code>Channel</code> are ready, and may then
take action on those that are</li>
<li>You get data in and out by passing a <code>Buffer</code> to the <code>Channel</code></li>
</ul>


<p>Here&rsquo;s an example based on TCP/IP Sockets in Java, a highly recommended book
about this stuff by Calvert and Donahoo.</p>

<p>```java
Selector slctr = Selector.open(); // factory
ServerSocketChannel chnl = ServerSocketChannel.open(); // factory
chnl.socket().bind(inetAddr); // set address to listen on</p>

<p>// For some reason Channels block by default. If we want to
// register with the Selector for notifications, we must turn
// that off.
chnl.configureBlocking(false);</p>

<p>// Notify Selector whenever this Channel has a new connection
// ready to be &ldquo;accepted&rdquo;. Such a notification still does
// <em>not</em> guarantee it will work immediately.
chnl.register(slctr, SelectionKey.OP_ACCEPT);</p>

<p>while (true) {</p>

<pre><code>// wait configurable period of time to be notified
// by any registered channel
int numNotifications = slctr.select(timeoutMS);
if (numNotifications == 0) {
    // We timed out without any notification.
    // We could do whatever we want here because we're
    // no longer blocked.
} else {
    // numNotifications different channels have notified us of
    // being available for Connect, Read, Accept, or Write.
    // It is OK to use these keys in concurrent threads.
    for (SelectionKey key : slctr.selectedKeys()) {
        // We're not sure which channel this key belonged to.
        // Also, notification was just a "hint" and we need to
        // check again whether the Channel is available.
        if (key.isAcceptable()) {
            // here's the actual call to accept()
            SocketChannel clientChnl =
                ((ServerSocketChannel) key.channel()).accept();

            // similar to the ServerSocketChannel
            clientChnl.configureBlocking(false);
            // Except that here we register to notify Selector
            // about being "readable", and
            clientChnl.register(
                key.selector(),
                SelectionKey.OP_READ,
                // We must associate an "attachment" with this
                // channel. This is the buffer that will be
                // filled with the incoming bytes rcvd via TCP.
                ByteBuffer.allocate(NUM_BYTES) // eg 256?
            );
        }
        if (key.isReadable()) {
            // retrieve the readable client socket's channel
            SocketChannel client =
                (SocketChannel) key.channel();

            // retrieve the ByteBuffer we associated with
            // that channel
            ByteBuffer buf = (ByteBuffer) key.attachment();

            // Attempt to read `buf.remaining()` bytes _from_
            // the Channel _into_ the ByteBuffer.
            int bytesRead = client.read(buf);

            // -1 from read() means end-of-stream, which in
            // this case means the client closed their output
            // side of the TCP connection. We may still be
            // able to send data if that side of the connection
            // has not been closed yet.
            if (bytesRead == -1) client.close();

            else if (bytesRead &gt; 0) {
                // if our application has data to write back
                // to the client, we must tell the selector
                // that we've now become interested in writing
                key.interestOps(SelectionKey.OP_READ
                                | SelectionKey.OP_WRITE);
            }
        }
        // socket not closed, and is writable
        if (key.isValid() &amp;&amp; key.isWritable()) {
            // beyond the scope of this post.
        }
    }
}
</code></pre>

<p>}
```</p>

<h2>Tips for Traps</h2>

<ul>
<li>Don&rsquo;t write to the network through a <code>PrintStream</code>

<ul>
<li>It chooses end-of-line chars based on your platform, not the protocol
(HTTP uses <code>\r\n</code>)</li>
<li>It uses the default char encoding of your platform (likely UTF-8), not
whatever the server expects (likely UTF-8)</li>
<li>It eats all exceptions into this <code>boolean checkError()</code> method, when
you&rsquo;re better off just using the normal exception hubbub</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intro to HTTP/2]]></title>
    <link href="http://ethanp.github.io/blog/2015/10/26/intro-to-http-slash-2/"/>
    <updated>2015-10-26T14:11:59-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/10/26/intro-to-http-slash-2</id>
    <content type="html"><![CDATA[<h2>The problem of HTTP/1.1</h2>

<h3>HTTP/1.1 is from an earlier Web</h3>

<p>For the past several years, Google, Mozilla, Akamai, the IETF, the academic
research community, and others have been engaged in efforts to reduce PLTs
experienced by users of the WWW. One bottleneck in apparent need of an update
is the now-&ldquo;ancient&rdquo; HTTP/1.1 (H1) protocol, published in 1999.</p>

<p>From the beginning, one of the major design goals of the original HTTP protocol
was simplicity to implement and adopt, to encourage growth of the WWW.
Evidently, this technique worked. However, over the past 16 years, the way
people create, distribute, and view pages on the WWW has changed drastically.
Pages today have far more Javascript, CSS, images, and other content to go
along with the vanilla HTML. In the course of this evolution, numerous issues
with H1 have come up, mostly pertaining to the number of sequential round-trips
it requires to fully download the data for each web page. These round-trips
introduce unnecessary latency.</p>

<!-- more -->


<p>Over the past 16 years, countermeasures, &ldquo;optional&rdquo; protocol alterations, and
application level workarounds have been suggested and implemented to reduce the
round-trip count and therefore latency of pages retrieved using H1.</p>

<h3>HTTP/1.1 PLT Optimizations</h3>

<p>Some optimizations servers and browsers use to lower PLTs seen by clients are
the following:</p>

<ol>
<li>Opening multiple TCP connections to request the download of multiple
required web page resources in parallel.</li>
<li>Inlining and concatenating scripts and stylesheets to reduce the number of
requests.</li>
<li><em>HTTP pipelining</em>, in which multiple requests are sent by the browser over a
single TCP connection before waiting for responses to any, then the server
sends all of the responses back in the same order.</li>
</ol>


<p>HTTP pipelining was an idea that did not work out. It seemed like a logical
approach to improve HTTP performance by reducing latency. However, in reality,
HOL blocking became an even greater issue then it already had been, and it also
led to issues with older proxies that are difficult to update. Nowadays, modern
browsers do not enable pipelining.</p>

<h2>HTTP/2 aims to address these issues</h2>

<p>The main reason multiple resources can&rsquo;t be <em>multiplexed</em> (downloaded in
parallel) over a single TCP connection in H1 is because it is an ASCII
protocol. Being an ASCII protocol means that there is no easy way to specify
how to demultiplex those responses with a parser.</p>

<p>To address this, HTTP/2 (H2) is a <em>binary</em> protocol that <em>can</em> easily multiplex
multiple requests and responses through a single TCP connection. It does this
using a new <em>binary framing layer</em>, in which responses are broken into separate
<em>frames</em> and interleaved through the TCP socket.</p>

<p>Web designers have discovered that to yield the best user experience, the
server should transmit certain most-important resources first, as soon as they
are available to send (e.g. loaded from disk). H2 frames (and <em>streams</em>) have
an (optional) <code>priority</code> field to allow the application developer (and server
writer) to bias the ordering of frames sent through the socket. For example,
one may want to ensure that <code>.html</code> files have a higher priority than <code>.jpeg</code>
files because <code>.jpeg</code>s contain a lot of data and are not as crucial for showing
the user a <em>barebones</em> version of the page they requested.</p>

<h3>Brief History</h3>

<p>The current H2 specification (RFC 7540) is based primarily on the SPDY protocol
invented and first used at Google. Google is in a rather unique position to
conduct research on this topic because they write the code running large
proportion of both user&rsquo;s browsers, <em>as well as</em> the servers for a large
portion of the web pages those people visit. This allows them to run real
controlled experimental studies on web traffic, to try to guage the best way to
speed up the Web.</p>

<p>Many have voiced concerns that Google&rsquo;s ideas are welcomed to quickly by the
standards committee. However, supporters are eager to get the ball rolling on
any good ideas, and point to the fact that Google already has <em>results</em>.</p>

<p>Note that some of the experimental results below were actually obtained using
SPDY rather than H2, but we will not concern ourselves with that because they
are the same in essence. Google Chrome still supports SPDY, but has claimed it
will remove support in 2016 so that there are not two competing
standardizations of basically the same protocol.</p>

<p>In reality, in my Chrome browser, using the &ldquo;HTTP/2 and SPDY indicator&rdquo;
extension, I can see that Google&rsquo;s search results and YouTube videos are served
over SPDY/3 running on top of their still-in-research QUIC protocol which is
meant to replace TCP as the transport layer protocol underneath HTTP. The
&ldquo;indicator&rdquo; reveals that many websites are in fact already being served over
H2, including Twitter. I hope to explore what exactly QUIC does, promises, and
delivers further in my research if there is time and space for it.</p>

<h3>Experimental Results</h3>

<p>Experiments investigating relative performance differences between H1 and H2
have yielded contradictory results (Wang et al.). In (Wang et al.), they found
that H2 generally outperforms H1, except when the task is to transmit <em>large</em>
objects over a <em>high-loss</em> connection. One of the goals in defining H2 is that
it is supposed to be easy to swap-in in-place of H1. This finding indicates
that especially with respect to mobile phones, the H1 optimization techniques
of inlining and concatenating web page resources is the wrong way to
improve performance for H2.</p>

<h3>HTTP for Mobile</h3>

<p>With respect to mobile devices, there are multiple problems with HTTP that are
<em>not</em> addressed by H2 (Erman et al.).</p>

<p>First of all, TCP congestion window part of congestion avoidance (<code>cwnd</code> and
<code>ssthresh</code>) makes it so that dropping a single TCP datagram leads to a
drastically reduced throughput. This is because TCP assumes the packet was
dropped due to network congestion, when in reality, it could have been for any
of the wireless-specific issues (multipath propagation, etc.)</p>

<p>Secondly, bad interactions between the 3G &amp; LTE MAC state machine timeouts and
modern default TCP timeouts and mechanisms lead to datagram <em>retransmissions</em>
with further resets on congestion window state variables, resulting in a
drastically negative impact on throughput.</p>

<h3>Research is needed</h3>

<p>From the system administrators' perspective, it is still difficult to decide
whether enabling H2 on one&rsquo;s servers is going to have a positive impact on
performance <em>at all</em> (Varvello et al.). The following questions still do not
have good enough answers to make answering that decision easy.</p>

<ol>
<li>What changes to the infrastructure and optimization pipeline are needed to
provide a significant benefit over H1?</li>
<li>What are the best algorithms for leveraging H2&rsquo;s new capabilities for
<em>server push</em> and <em>prioritization</em>?</li>
<li>Why do experiments by different research groups yield such a wide disparity
in results?</li>
<li>Are we unlikely to see any real PLT improvements until we improve the
underlying transport layer protocol?</li>
<li>In what ways is the problem different for mobile?

<ul>
<li>What changes need to be made in the mobile-specific scenario, viz. where
interference, low bandwidth, high latency, and battery life
considerations are major concerns?</li>
</ul>
</li>
</ol>


<h2>Glossary</h2>

<ul>
<li><strong>IETF</strong> (Internet Engineering Task Force) &mdash; a standards organization for
the Internet, which produces &ldquo;RFC"s (requests for comment) specifying some of
the crucial internet protocols, such as HTTP, TCP, and TLS.</li>
<li><strong>PLT</strong> (Page Load Time) &mdash; the duration between the time at which a user
submits a request for a web page, and the time at which she receives the last
byte of data needed to represent that web page correctly</li>
<li><strong>WWW</strong> (World Wide Web) &mdash; a decentralized collection of resources,
requested and transmitted using <em>HTTP</em></li>
<li><strong>HTTP</strong> (HyperText Transfer Protocol) &mdash; a protocol specifying the way an
HTTP <em>client</em> may upload, download, update, etc. documents on an HTTP
<em>server</em>. Often used for downloading <em>HTML</em> in particular.</li>
<li><strong>TCP</strong> (Transmission Control Protocol) &mdash; a protocol which provides an
application the abstraction of having a reliable pipe across the Internet
into/from which it may send/receive ordered sequences of bytes</li>
<li><strong>HOL blocking</strong> (head-of-line blocking) &mdash; when multiple messages are being
sent, but due to imposed FIFO ordering, messages ready to be sent must wait
for an earlier message which is taking a long time</li>
</ul>


<h2>References</h2>

<ul>
<li>Erman, Jeffrey, et al. &ldquo;Towards a SPDY'ier mobile web?.&rdquo; <em>Proceedings of the
ninth ACM conference on Emerging networking experiments and technologies.
ACM</em>, 2013.</li>
<li>Grigorik, Ilya. High Performance Browser Networking: What every web developer
should know about networking and web performance. <em>O'Reilly Media, Inc.</em>, 2013.</li>
<li>Roth, Gregor. &ldquo;HTTP/2 for Java Developers&rdquo;. Retrieved from
<a href="http://www.javaworld.com/article/2916548/java-web-development/http-2-for-">http://www.javaworld.com/article/2916548/java-web-development/http-2-for-</a>
java-developers.html on October 8, 2015.</li>
<li>Stenberg, Daniel aka. bagder. <em>http2 explained</em>, retrieved October 8, 2015;
downloaded as MOBI.</li>
<li>Varvello, Matteo, et al. &ldquo;To HTTP/2, or Not To HTTP/2, That Is The Question&rdquo;
arXiv preprint arXiv:1507.06562 (2015).</li>
<li>Wang, Xiao Sophia, et al. &ldquo;How speedy is SPDY.&rdquo; <em>Proc. of the 11th USENIX
Symposium on Networked Systems Design and Implementation (NSDI)</em>. 2014.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why the WiFi Sucks on the Porch]]></title>
    <link href="http://ethanp.github.io/blog/2015/09/24/why-the-wifi-sucks-on-the-porch/"/>
    <updated>2015-09-24T14:31:30-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/09/24/why-the-wifi-sucks-on-the-porch</id>
    <content type="html"><![CDATA[<h2>Why is WiFi so slow on the porch?</h2>

<p>The problem at my house is that WiFi connectivity is generally fine inside the
house, but while sitting on the porch it is nearly impossible for anyone to
browse the Hinternets. I think a lesson of my <a href="http://www.cs.utexas.edu/~lili/classes/F15-CS386W/">Wireless Networking</a> course
explains this observation.</p>

<p>In Wireless networking we have the situation called the <a href="http://www.wikiwand.com/en/Hidden_node_problem"><strong>Hidden Terminal
Problem</strong></a>, which is the following. Node <code>A</code> is too far from node <code>C</code> to
hear its transmissions, but both are in range of node <code>B</code> which sits between
<code>A</code> and <code>C</code>. Suppose <code>C</code> is currently transmitting data to <code>B</code>. Now <code>A</code> checks
whether any transmissions are currently happening, and finds that there are
none (because it is out of range of <code>C</code>). So <code>A</code> goes ahead and sends data to
<code>B</code>. Now <code>B</code> can&rsquo;t understand either data packet because they collided and
interfered with each other in an unrecoverable way because they were both sent
in the same channel.</p>

<p>I think the porch scenario is simply an example of the <em>Hidden Terminal
Problem</em> above. In my room, my laptop can hear many of my neighbors' WiFi LAN
networks, but it&rsquo;s the same set that my WiFi router sitting in the closet can
hear. However outside, where there&rsquo;s less cause for signal attenuation, my
laptop can hear many more WiFi LAN networks than the router inside. So the
router checks whether there is congestion, doesn&rsquo;t hear any, and sends the
data. But there <em>is</em>, in fact, congestion, and my computer doesn&rsquo;t receive the
data properly. The data is therefore not <em>ACKd</em>, the router times out on the
<em>ACK</em>, and has to retransmit, and so on. This makes for a <em>far</em> slower
Hinterconnectivity outside on the porch than in my room.</p>

<p>Maybe the lesson learnt is that the WiFi router should be situated in a place
where it can hear more of the outside noise so that it can compensate better
for that noise.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Basics of Wireless Communication]]></title>
    <link href="http://ethanp.github.io/blog/2015/09/07/basics-of-wireless-communication/"/>
    <updated>2015-09-07T15:45:58-07:00</updated>
    <id>http://ethanp.github.io/blog/2015/09/07/basics-of-wireless-communication</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been doing the readings for my <a href="http://www.cs.utexas.edu/~lili/classes/F15-CS386W/">Wireless Networking course</a> at
UTexas, and in the process have dug into much of the basics of radios and
networks that I had ignored in the past. Here, I will try briefly describe what
I have learned. Maybe not everything I will say here is exactly correct, but I
think it&rsquo;s at least <em>mostly</em> correct.</p>

<p>Let&rsquo;s try to start somewhere near the beginning. Our <strong>goal</strong> is to transfer a
<em>information</em> from one location <code>LocSND</code> to another <code>LocRCV</code> <em>conveniently</em>.
The way we will accomplish that is by having <code>LocSND</code> manipulate the
<em>electromagnetic field</em> around <code>LocRCV</code>. More specifically, we will <em>encode</em> a
binary <em>dataframe</em> as <em>modulations</em> of a <em>radio signal</em> around a pre-determined
<em>carrier frequency</em>.</p>

<p>How do we <em>do</em> that?</p>

<!-- more -->


<p>We use an LRC circuit to make electrons oscillate in an
antenna. These oscillating electrons ram into loose and excitable electrons in
the antenna&rsquo;s metal, this releases a photon at a particular frequency. Globally
(i.e. within the entire transmitting antenna), enough photons are being
released that it seems to an external observer looking at the produced
electromagnetic (EM) field like there is a continuous signal being emitted.</p>

<p>So we&rsquo;re sending these EM ripples, which are generally at our carrier
frequency. However, if we just sent a basic frequency, there would be no
<em>information</em> in there, so we have to <em>modulate</em> it. We can modulate its
amplitude (A), phase (phi), and frequency (omega), the 3 free
parameters of the equation (in the top left of the equation in the following
gif from &ldquo;sengpielaudio&rdquo;)</p>

<p><img src="http://www.sengpielaudio.com/Sinuskurve01.gif" alt="sine wave" /></p>

<p>This would give us
1. <strong>Carrier frequency</strong> &mdash;&ndash; the EM frequency <em>inside</em> which our signal is
  encoded
2. <strong>amplitude shift keying (ASK)</strong> &mdash; send signal at <em>carrier frequency</em> by
   modulating the signal&rsquo;s <em>amplitude</em>
3. <strong>frequency shift keying (FSK)</strong> &mdash; similar but modulates <em>frequency</em>
4. <strong>phase shift keying (PSK)</strong> &mdash; again, but modulates <em>phase</em></p>

<p>One simple method would be to say our carrier frequency is 5 Hz, but our band
is actually [4,6] Hz. So whenever the signal is 4 Hz, that means I&rsquo;m sending a
0, and if the signal is 6 Hz, it means I&rsquo;m sending a 1, and a new digit starts
every 1 ms. That would be an example of <strong>FSK</strong>.</p>

<p>A fundamental problem that we must solve is that all senders and receivers of
information via EM fields with their antenna(s) are sharing the a single
<em>medium</em> for transmitting that field (viz. the air, etc.). So if <code>LocSND</code> sends
a message to <code>LocRCV_1</code>, then <code>LocRCV_2</code> sitting one foot away can hear that
message loud and clear. This leads to three major issues: <strong>security</strong>,
<strong>multiplexing</strong>, and <strong>interference</strong>.</p>

<p>To <strong>multiplex</strong> means to send multiple distinct signals through a single
channel. How are we going to send distinct signals to receivers 1 and 2 in such
a way that both can understand the signal meant for them? We can chop up the
frequency band that our transmitter can use into 2 smaller bands, and use each
of those bands as separate carriers. Then we tune the receivers to pick up
frequencies in their respective bands. This is what is called <strong>frequency
division multiplexing (FDM)</strong>, but we can also multiplex across space, time,
and <em>code</em>.</p>

<p>Of course, I&rsquo;ve only scratched the very surface of what&rsquo;s going on here, but
that&rsquo;s all the time I&rsquo;ve got.</p>
]]></content>
  </entry>
  
</feed>
